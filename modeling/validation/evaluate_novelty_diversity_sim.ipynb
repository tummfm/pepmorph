{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Iterable, Optional\n",
    "import pickle\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"font.family\"] = \"Lato\"\n",
    "rcParams[\"font.size\"] = 16\n",
    "rcParams['text.color'] = '#3F3F3F'\n",
    "rcParams['axes.labelcolor'] = '#3F3F3F'\n",
    "rcParams['xtick.color'] = '#3F3F3F'\n",
    "rcParams['ytick.color'] = '#3F3F3F'\n",
    "TEAL = [\"#b5d1ae\", \"#80ae9a\", \"#568b87\", \"#326b77\", \"#1b485e\", \"#122740\"]\n",
    "\n",
    "FEATURES = [\"length\",\"is_assembled\",\"ap\",\"has_beta_sheet_content\",\"hydrophobic_moment\",\"net_charge\"]\n",
    "COUNT_FEATURES = FEATURES\n",
    "MORPH_FEATURES = [\"has_beta_sheet_content\",\"hydrophobic_moment\",\"net_charge\"]\n",
    "\n",
    "from classifier.models import PeptidePredictor\n",
    "from cvae.models import CVAESimpleEnc\n",
    "from cvae.utils import (\n",
    "    CONDITION_LENGTH, MAX_SEQ_LENGTH, MAX_FASTA_LENGTH, ALPHABET, PAD_TOKEN_ID, EOS_ID, BOS_ID, idx_to_fasta, esm_model_pretrained, convert_and_pad\n",
    ")\n",
    "\n",
    "try:\n",
    "    import editdistance\n",
    "except ImportError:\n",
    "    editdistance = None\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "    NUMBA_AVAILABLE = True\n",
    "except Exception:\n",
    "    NUMBA_AVAILABLE = False\n",
    "\n",
    "LENGTHS = list(range(5, 11))\n",
    "RNG = np.random.default_rng(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device, torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "df = pd.read_csv(\"../../clean_data/merged_all.csv\", keep_default_na=False, na_values=[''])\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.1, stratify=df['length'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1, stratify=train_val_df['length'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device, torch.cuda.device_count())\n",
    "\n",
    "cvae_model = CVAESimpleEnc(\n",
    "    encoder_hidden_dim=256,\n",
    "    num_encoder_layers=2,\n",
    "    vocab_size=len(ALPHABET),\n",
    "    latent_dim=24,\n",
    "    cond_dim=CONDITION_LENGTH,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    decoder_hidden_dim=256,\n",
    "    num_decoder_layers=2,\n",
    "    nhead=8,\n",
    "    dropout=0.1)\n",
    "\n",
    "pretrained_state_dict = torch.load(\"../cvae/chkpts/finetuned_cvae.pt\", map_location=device, weights_only=True)\n",
    "cvae_model.load_state_dict(pretrained_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVAE initialized with finetuned architecture\n"
     ]
    }
   ],
   "source": [
    "cvae_model.eval()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    cvae_model = nn.DataParallel(cvae_model)\n",
    "\n",
    "cvae_model.to(device)\n",
    "print(\"CVAE initialized with finetuned architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_fasta(tokens):\n",
    "    fasta = ''\n",
    "    for token in tokens:\n",
    "        if token == 2:\n",
    "            break\n",
    "        if token in [0, 1, 3, 29, 30, 31, 32]:\n",
    "            continue\n",
    "        else:\n",
    "            fasta += idx_to_fasta[token]\n",
    "    return fasta    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\"length\",\"is_assembled\",\"ap\",\"has_beta_sheet_content\",\"hydrophobic_moment\",\"net_charge\"]\n",
    "FEATURES_ALL = FEATURES\n",
    "COND_FEATURES = [\"is_assembled\",\"ap\",\"has_beta_sheet_content\",\"hydrophobic_moment\",\"net_charge\"]\n",
    "AMINO_ALPHABET = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "AA_TO_INT = {aa:i for i,aa in enumerate(AMINO_ALPHABET)}\n",
    "PAD_VAL = -1\n",
    "\n",
    "def encode_seq(s: str) -> np.ndarray:\n",
    "    return np.array([AA_TO_INT.get(ch, len(AMINO_ALPHABET)-1) for ch in s], dtype=np.int8)\n",
    "\n",
    "def encode_pad_sequences(seqs):\n",
    "    \"\"\"Return (codes[N, Lmax] int16, lens[N] int16) padded with PAD_VAL.\"\"\"\n",
    "    lens = np.array([len(s) for s in seqs], dtype=np.int16)\n",
    "    Lmax = int(lens.max()) if len(lens) else 0\n",
    "    codes = np.full((len(seqs), Lmax), PAD_VAL, dtype=np.int16)\n",
    "    for i, s in enumerate(seqs):\n",
    "        for j, ch in enumerate(s):\n",
    "            codes[i, j] = AA_TO_INT.get(ch, len(AMINO_ALPHABET)-1)\n",
    "    return codes, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needleman–Wunsch % identity (global)\n",
    "if NUMBA_AVAILABLE:\n",
    "    @njit(cache=True, fastmath=True)\n",
    "    def nw_identity_counts(a_codes: np.ndarray, b_codes: np.ndarray) -> Tuple[int,int]:\n",
    "        na, nb = a_codes.size, b_codes.size\n",
    "        score = np.empty((na+1, nb+1), dtype=np.int16)\n",
    "        match = np.zeros((na+1, nb+1), dtype=np.int16)\n",
    "        alen  = np.zeros((na+1, nb+1), dtype=np.int16)\n",
    "\n",
    "        gap = -1\n",
    "        score[0,0] = 0\n",
    "        for i in range(1, na+1):\n",
    "            score[i,0] = score[i-1,0] + gap\n",
    "            match[i,0] = 0\n",
    "            alen[i,0]  = i\n",
    "        for j in range(1, nb+1):\n",
    "            score[0,j] = score[0,j-1] + gap\n",
    "            match[0,j] = 0\n",
    "            alen[0,j]  = j\n",
    "\n",
    "        for i in range(1, na+1):\n",
    "            ai = a_codes[i-1]\n",
    "            for j in range(1, nb+1):\n",
    "                bj = b_codes[j-1]\n",
    "                sc_diag = score[i-1,j-1] + (1 if ai==bj else 0)\n",
    "                mt_diag = match[i-1,j-1] + (1 if ai==bj else 0)\n",
    "                ln_diag = alen[i-1,j-1] + 1\n",
    "\n",
    "                sc_up = score[i-1,j] + gap\n",
    "                mt_up = match[i-1,j]\n",
    "                ln_up = alen[i-1,j] + 1\n",
    "\n",
    "                sc_left = score[i,j-1] + gap\n",
    "                mt_left = match[i,j-1]\n",
    "                ln_left = alen[i,j-1] + 1\n",
    "\n",
    "                # choose best by (score, matches, -alen)\n",
    "                sc = sc_diag; mt = mt_diag; ln = ln_diag\n",
    "\n",
    "                if (sc_up > sc) or (sc_up == sc and (mt_up > mt or (mt_up == mt and ln_up < ln))):\n",
    "                    sc = sc_up; mt = mt_up; ln = ln_up\n",
    "\n",
    "                if (sc_left > sc) or (sc_left == sc and (mt_left > mt or (mt_left == mt and ln_left < ln))):\n",
    "                    sc = sc_left; mt = mt_left; ln = ln_left\n",
    "\n",
    "                score[i,j] = sc; match[i,j] = mt; alen[i,j] = ln\n",
    "\n",
    "        return int(match[na,nb]), int(alen[na,nb])\n",
    "\n",
    "else:\n",
    "    def nw_identity_counts(a_codes: np.ndarray, b_codes: np.ndarray) -> Tuple[int,int]:\n",
    "        na, nb = a_codes.size, b_codes.size\n",
    "        gap = -1\n",
    "\n",
    "        score = [[0]*(nb+1) for _ in range(na+1)]\n",
    "        match = [[0]*(nb+1) for _ in range(na+1)]\n",
    "        alen  = [[0]*(nb+1) for _ in range(na+1)]\n",
    "\n",
    "        for i in range(1, na+1):\n",
    "            score[i][0] = score[i-1][0] + gap\n",
    "            alen[i][0]  = i\n",
    "        for j in range(1, nb+1):\n",
    "            score[0][j] = score[0][j-1] + gap\n",
    "            alen[0][j]  = j\n",
    "\n",
    "        for i in range(1, na+1):\n",
    "            ai = a_codes[i-1]\n",
    "            for j in range(1, nb+1):\n",
    "                bj = b_codes[j-1]\n",
    "                sc_diag = score[i-1][j-1] + (1 if ai==bj else 0)\n",
    "                mt_diag = match[i-1][j-1] + (1 if ai==bj else 0)\n",
    "                ln_diag = alen[i-1][j-1] + 1\n",
    "\n",
    "                sc_up = score[i-1][j] + gap\n",
    "                mt_up = match[i-1][j]\n",
    "                ln_up = alen[i-1][j] + 1\n",
    "\n",
    "                sc_left = score[i][j-1] + gap\n",
    "                mt_left = match[i][j-1]\n",
    "                ln_left = alen[i][j-1] + 1\n",
    "\n",
    "                sc = sc_diag; mt = mt_diag; ln = ln_diag\n",
    "                if (sc_up > sc) or (sc_up == sc and (mt_up > mt or (mt_up == mt and ln_up < ln))):\n",
    "                    sc = sc_up; mt = mt_up; ln = ln_up\n",
    "                if (sc_left > sc) or (sc_left == sc and (mt_left > mt or (mt_left == mt and ln_left < ln))):\n",
    "                    sc = sc_left; mt = mt_left; ln = ln_left\n",
    "\n",
    "                score[i][j] = sc; match[i][j] = mt; alen[i][j] = ln\n",
    "\n",
    "        return match[na][nb], alen[na][nb]\n",
    "\n",
    "def nw_percent_identity(a: str, b: str) -> float:\n",
    "    ac, bc = encode_seq(a), encode_seq(b)\n",
    "    m, L = nw_identity_counts(ac, bc)\n",
    "    return 1.0 if L == 0 else (m / L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_edit_dist(a: str, b: str) -> float:\n",
    "    d = editdistance.eval(a, b)\n",
    "    return d / max(len(a), len(b), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _even_counts_over_k(n_total: int, k_min: int = 1, k_max: int = 6, rng: np.random.RandomState = np.random.RandomState(0)) -> Dict[int,int]:\n",
    "    \"\"\"\n",
    "    Evenly split n_total over k in [k_min..k_max]. Any remainder is assigned randomly.\n",
    "    Returns {k_used: count}.\n",
    "    \"\"\"\n",
    "    Ks = list(range(k_min, k_max+1))\n",
    "    base = n_total // len(Ks)\n",
    "    rem  = n_total % len(Ks)\n",
    "    counts = {k: base for k in Ks}\n",
    "    if rem > 0:\n",
    "        bump = rng.choice(Ks, size=rem, replace=False)\n",
    "        for k in bump:\n",
    "            counts[k] += 1\n",
    "    return counts\n",
    "\n",
    "def _random_k_subsets(features: List[str], k_list: List[int], rng: np.random.RandomState) -> List[List[str]]:\n",
    "    \"\"\"For each k in k_list, sample a random k-subset of `features` (without replacement).\"\"\"\n",
    "    out = []\n",
    "    for k in k_list:\n",
    "        out.append(rng.choice(features, size=k, replace=False).tolist())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COND_FEATURES = [\"is_assembled\",\"ap\",\"has_beta_sheet_content\",\"hydrophobic_moment\",\"net_charge\"]\n",
    "\n",
    "def fit_gmm_per_length(train_df: pd.DataFrame,\n",
    "                       lengths: Iterable[int],\n",
    "                       n_components: int = 5,\n",
    "                       random_state: int = 42):\n",
    "    \"\"\"Fit per-length imputers, scalers, and GMMs on descriptor space (excl. length).\"\"\"\n",
    "    models = {}\n",
    "    for L in lengths:\n",
    "        dfL = train_df.loc[train_df[\"length\"] == L, COND_FEATURES].copy()\n",
    "        if dfL.empty:\n",
    "            raise ValueError(f\"No training rows for length={L}\")\n",
    "        num_cols = [\"ap\",\"hydrophobic_moment\",\"net_charge\"]\n",
    "        bin_cols = [\"is_assembled\",\"has_beta_sheet_content\"]\n",
    "        ct = ColumnTransformer([\n",
    "            (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
    "            (\"bin\", SimpleImputer(strategy=\"most_frequent\"), bin_cols),\n",
    "        ])\n",
    "        X_imp = ct.fit_transform(dfL)\n",
    "\n",
    "        scaler = MinMaxScaler((0,1))\n",
    "        X_scaled = scaler.fit_transform(X_imp)\n",
    "\n",
    "        gmm = GaussianMixture(n_components=n_components, covariance_type=\"full\",\n",
    "                              random_state=random_state)\n",
    "        gmm.fit(X_scaled)\n",
    "        models[L] = {\"imputer\": ct, \"scaler\": scaler, \"gmm\": gmm,\n",
    "                     \"num_cols\": num_cols, \"bin_cols\": bin_cols}\n",
    "    return models\n",
    "\n",
    "def sample_in_distribution_conditions(models: Dict[int,dict],\n",
    "                                      per_length_counts: Dict[int,int],\n",
    "                                      rng: np.random.RandomState = np.random.RandomState(0),\n",
    "                                      global_even_k: bool = True) -> List[dict]:\n",
    "    \"\"\"\n",
    "    For each length L, draw per_length_counts[L] GMM samples.\n",
    "    If global_even_k=True, split ALL those samples evenly over k∈{1..6}; otherwise do per-length.\n",
    "    \"\"\"\n",
    "    conds = []\n",
    "\n",
    "    perL_params = {}\n",
    "    for L, count in per_length_counts.items():\n",
    "        pack = models[L]\n",
    "        gmm, scaler = pack[\"gmm\"], pack[\"scaler\"]\n",
    "        num, binf   = pack[\"num_cols\"], pack[\"bin_cols\"]\n",
    "\n",
    "        Xs, _ = gmm.sample(count)\n",
    "        X_imp = scaler.inverse_transform(np.clip(Xs, 0, 1))\n",
    "\n",
    "        params_list = []\n",
    "        for row in X_imp:\n",
    "            row = np.clip(row, 0, None)\n",
    "            vals = {}\n",
    "            off = 0\n",
    "            for i, c in enumerate(num):\n",
    "                vals[c] = float(row[off+i])\n",
    "            off += len(num)\n",
    "            for i, c in enumerate(binf):\n",
    "                vals[c] = float(1.0 if row[off+i] >= 0.5 else 0.0)\n",
    "            vals_full = {\"length\": int(L)}\n",
    "            vals_full.update(vals)\n",
    "            params_list.append(vals_full)\n",
    "\n",
    "        perL_params[L] = params_list\n",
    "\n",
    "    Ks = list(range(1, 7))\n",
    "    total = sum(len(v) for v in perL_params.values())\n",
    "\n",
    "    def _even_counts_over_k(n_total: int) -> dict:\n",
    "        base = n_total // len(Ks)\n",
    "        rem  = n_total %  len(Ks)\n",
    "        counts = {k: base for k in Ks}\n",
    "        if rem > 0:\n",
    "            for i in range(rem):\n",
    "                counts[Ks[i]] += 1\n",
    "        return counts\n",
    "\n",
    "    counts_by_k = _even_counts_over_k(total)\n",
    "    k_list = []\n",
    "    for k in Ks:\n",
    "        k_list += [k]*counts_by_k[k]\n",
    "    rng.shuffle(k_list)\n",
    "\n",
    "    ptr = 0\n",
    "    for L in sorted(perL_params.keys()):\n",
    "        params_list = perL_params[L]\n",
    "        k_slice = k_list[ptr:ptr+len(params_list)]\n",
    "        ptr += len(params_list)\n",
    "\n",
    "        used_subsets = _random_k_subsets(FEATURES_ALL, k_slice, rng)\n",
    "        for row_params, used_feats in zip(params_list, used_subsets):\n",
    "            conds.append({\n",
    "                \"params\": row_params,\n",
    "                \"length\": L,\n",
    "                \"used_features\": used_feats,\n",
    "                \"ood_type\": \"in_dist\",\n",
    "            })\n",
    "\n",
    "    return conds\n",
    "\n",
    "def compute_rare_values(train_df: pd.DataFrame, L: int) -> dict:\n",
    "    \"\"\"Pick rare values for has_beta_sheet_content (least frequent class) and\n",
    "       rare extreme for hydrophobic_moment (min/max depending on rarity).\"\"\"\n",
    "    dfL = train_df.loc[train_df[\"length\"] == L]\n",
    "    freq = dfL[\"has_beta_sheet_content\"].value_counts(dropna=True)\n",
    "    if len(freq) == 0:\n",
    "        rare_beta = 1.0\n",
    "    else:\n",
    "        rare_beta = float(freq.idxmin())\n",
    "    # extreme\n",
    "    q05, q95 = dfL[\"hydrophobic_moment\"].quantile(0.05), dfL[\"hydrophobic_moment\"].quantile(0.95)\n",
    "    tail_low = (dfL[\"hydrophobic_moment\"] <= q05).sum()\n",
    "    tail_high = (dfL[\"hydrophobic_moment\"] >= q95).sum()\n",
    "    rare_hm = float(q05 if tail_low < tail_high else q95)\n",
    "    print(f\"Length {L}: rare beta={rare_beta}, rare hm={rare_hm} (tails {tail_low}/{tail_high})\")\n",
    "    return {\"has_beta_sheet_content\": rare_beta, \"hydrophobic_moment\": rare_hm}\n",
    "\n",
    "def sample_ood_conditions(train_df: pd.DataFrame,\n",
    "                          lengths: Iterable[int],\n",
    "                          total_ood: int = 10,\n",
    "                          rng: np.random.RandomState = np.random.RandomState(1)) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Create OOD conditions targeting rare beta flag and rare hydrophobic moment extremes.\n",
    "    For each OOD condition, pick k_used ∈ {1..6} at random, BUT force the targeted feature to be included.\n",
    "    Other features set to per-length medians (except the targeted rare one).\n",
    "    \"\"\"\n",
    "    lengths = list(lengths)\n",
    "    total_ood = max(2, total_ood)\n",
    "    half = total_ood // 2\n",
    "    idx_beta = rng.choice(lengths, size=half, replace=True)\n",
    "    idx_hm   = rng.choice(lengths, size=total_ood - half, replace=True)\n",
    "\n",
    "    conds = []\n",
    "    # Rare beta\n",
    "    for L in idx_beta:\n",
    "        rare = compute_rare_values(train_df, L)\n",
    "        params = {\n",
    "            \"length\": int(L),\n",
    "            \"has_beta_sheet_content\": rare[\"has_beta_sheet_content\"],\n",
    "            \"is_assembled\": float(train_df.loc[train_df.length==L,\"is_assembled\"].median()),\n",
    "            \"ap\": float(train_df.loc[train_df.length==L,\"ap\"].median()),\n",
    "            \"hydrophobic_moment\": float(train_df.loc[train_df.length==L,\"hydrophobic_moment\"].median()),\n",
    "            \"net_charge\": float(train_df.loc[train_df.length==L,\"net_charge\"].median()),\n",
    "        }\n",
    "        k = int(rng.randint(1, 7))\n",
    "        used = set([\"has_beta_sheet_content\"])\n",
    "        if k > 1:\n",
    "            pool = [f for f in FEATURES_ALL if f not in used]\n",
    "            used.update(rng.choice(pool, size=k-1, replace=False).tolist())\n",
    "        conds.append({\n",
    "            \"params\": params,\n",
    "            \"length\": L,\n",
    "            \"used_features\": sorted(list(used)),\n",
    "            \"ood_type\": \"rare_beta\"\n",
    "        })\n",
    "\n",
    "    # Rare HM\n",
    "    for L in idx_hm:\n",
    "        rare = compute_rare_values(train_df, L)\n",
    "        params = {\n",
    "            \"length\": int(L),\n",
    "            \"hydrophobic_moment\": rare[\"hydrophobic_moment\"],\n",
    "            \"is_assembled\": float(train_df.loc[train_df.length==L,\"is_assembled\"].median()),\n",
    "            \"ap\": float(train_df.loc[train_df.length==L,\"ap\"].median()),\n",
    "            \"has_beta_sheet_content\": float(train_df.loc[train_df.length==L,\"has_beta_sheet_content\"].median()),\n",
    "            \"net_charge\": float(train_df.loc[train_df.length==L,\"net_charge\"].median()),\n",
    "        }\n",
    "        k = int(rng.randint(1, 7))\n",
    "        used = set([\"hydrophobic_moment\"])\n",
    "        if k > 1:\n",
    "            pool = [f for f in FEATURES_ALL if f not in used]\n",
    "            used.update(rng.choice(pool, size=k-1, replace=False).tolist())\n",
    "        conds.append({\n",
    "            \"params\": params,\n",
    "            \"length\": L,\n",
    "            \"used_features\": sorted(list(used)),\n",
    "            \"ood_type\": \"rare_hm\"\n",
    "        })\n",
    "\n",
    "    return conds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cond_mask_vectors(device, **kwargs):\n",
    "    condition = torch.zeros(len(FEATURES), dtype=torch.float, device=device)\n",
    "    mask = torch.zeros(len(FEATURES), dtype=torch.float, device=device)\n",
    "\n",
    "    for idx, feature in enumerate(FEATURES):\n",
    "        value = kwargs.get(feature, None)\n",
    "        if value is not None:\n",
    "            condition[idx] = value if feature != 'length' else value / MAX_FASTA_LENGTH\n",
    "            mask[idx] = 1.0\n",
    "    return condition, mask\n",
    "\n",
    "def sample_from_prior_ar(\n",
    "    model, condition, mask,\n",
    "    temperature=1.0,\n",
    "    n=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    1) z ~ p(z|c) instead of N(0,I)\n",
    "    2) Autoregressively decode, starting from BOS, stopping on EOS or max `max_length`\n",
    "    \"\"\"\n",
    "    m = model.module if hasattr(model, \"module\") else model\n",
    "    m.eval()\n",
    "\n",
    "    if n == 1:\n",
    "        condition = condition.unsqueeze(0)\n",
    "        mask      = mask.unsqueeze(0)\n",
    "    else:\n",
    "        condition = condition.repeat(n, 1)\n",
    "        mask      = mask.repeat(n, 1)\n",
    "\n",
    "    device = next(m.parameters()).device\n",
    "    condition, mask = condition.to(device), mask.to(device)\n",
    "    B = condition.size(0)\n",
    "    summary = m.compute_summary(condition, mask)\n",
    "    prior_mu, prior_logvar = m.compute_prior(summary)\n",
    "    z = m.reparameterize(prior_mu, prior_logvar)\n",
    "\n",
    "    seq = torch.full((B, 1), BOS_ID,\n",
    "                     dtype=torch.long, device=device)\n",
    "\n",
    "    finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
    "    outputs  = torch.full((B, MAX_SEQ_LENGTH), PAD_TOKEN_ID,\n",
    "                          dtype=torch.long, device=device)\n",
    "\n",
    "    for t in range(MAX_SEQ_LENGTH):\n",
    "        logits = m.decode(z, seq, summary)\n",
    "\n",
    "        next_logits = logits[:, -1, :]\n",
    "        if temperature == 1.0:\n",
    "            next_token = next_logits.argmax(dim=-1, keepdim=True)\n",
    "        else:\n",
    "            probs = torch.nn.functional.softmax(next_logits / temperature, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        outputs[:, t] = next_token.squeeze(1)\n",
    "\n",
    "        seq = torch.cat([seq, next_token], dim=1)\n",
    "\n",
    "        finished = finished | (next_token.squeeze(1) == EOS_ID)\n",
    "        if finished.all():\n",
    "            break\n",
    "\n",
    "    outs = []\n",
    "    for i, row in enumerate(outputs.tolist()):\n",
    "        if EOS_ID in row:\n",
    "            cut = row.index(EOS_ID)\n",
    "            toks = row[:cut]\n",
    "        else:\n",
    "            toks = row\n",
    "        fasta = to_fasta(toks)\n",
    "        outs.append((fasta, len(fasta)))#, cond_pred_from_prior[i, :].cpu().tolist()))\n",
    "\n",
    "    return outs\n",
    "\n",
    "def generate_for_conditions(cvae_model,\n",
    "                            device,\n",
    "                            cond_records: List[dict],\n",
    "                            n_per_cond: int = 100,\n",
    "                            temperature: float = 1.0) -> List[dict]:\n",
    "    \"\"\"\n",
    "    For each condition (with params for all 6 descriptors), only pass 'used_features' into\n",
    "    generate_cond_mask_vectors to set the mask appropriately.\n",
    "    \"\"\"\n",
    "    all_samples = []\n",
    "    for idx, rec in enumerate(tqdm(cond_records, desc=\"Generating sequences per condition\")):\n",
    "        params_full = rec[\"params\"]\n",
    "        used = set(rec.get(\"used_features\", FEATURES_ALL))\n",
    "\n",
    "        kwargs = {k: v for k, v in params_full.items() if k in used}\n",
    "\n",
    "        cond_vec, mask_vec = generate_cond_mask_vectors(device=device, **kwargs)\n",
    "        outs = sample_from_prior_ar(cvae_model, cond_vec, mask_vec,\n",
    "                                    temperature=temperature, n=n_per_cond)\n",
    "        for fasta, L in outs:\n",
    "            all_samples.append({\n",
    "                \"cond_idx\": idx,\n",
    "                \"sequence\": fasta,\n",
    "                \"length\": L,\n",
    "                \"target_length\": params_full[\"length\"],\n",
    "                \"params\": params_full,\n",
    "                \"used_features\": sorted(list(used)),\n",
    "                \"ood_type\": rec.get(\"ood_type\", \"in_dist\")\n",
    "            })\n",
    "    return all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact_novelty_and_nn_edit(generated: List[str],\n",
    "                                      training: List[str],\n",
    "                                      length_window: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each generated sequence: exact match vs training (bool),\n",
    "    and nearest-train normalized edit distance (min over a length bucket).\n",
    "    length_window=1 compares against training lengths in [len-1, len, len+1] to save time.\n",
    "    \"\"\"\n",
    "    train_by_len = defaultdict(list)\n",
    "    for t in training:\n",
    "        train_by_len[len(t)].append(t)\n",
    "\n",
    "    rows = []\n",
    "    print(\"Calculating exact novelty + nearest-train normalized edit distance...\")\n",
    "    for seq in tqdm(generated, desc=\"Novelty/NN-edit\"):\n",
    "        L = len(seq)\n",
    "        exact = (seq in train_by_len[L])\n",
    "        pool = []\n",
    "        for dL in range(-length_window, length_window+1):\n",
    "            pool.extend(train_by_len.get(L+dL, []))\n",
    "        if not pool:\n",
    "            nn = min((normalized_edit_dist(seq, t) for t in training))\n",
    "        else:\n",
    "            nn = min((normalized_edit_dist(seq, t) for t in pool))\n",
    "        rows.append((seq, exact, nn))\n",
    "    return pd.DataFrame(rows, columns=[\"sequence\",\"exact_match\",\"nn_dist\"])\n",
    "\n",
    "def compute_diversity_and_uniqueness(samples_df: pd.DataFrame,\n",
    "                                     across_pairs_sample: int = 100000) -> dict:\n",
    "    \"\"\"\n",
    "    Diversity = pairwise normalized Levenshtein distances.\n",
    "    - Within-condition: compute all pairs per condition (100 -> 4950 pairs each).\n",
    "    - Across-conditions: random sample of pairs with different cond_idx.\n",
    "    Uniqueness: % unique sequences overall and per condition.\n",
    "    \"\"\"\n",
    "    all_seqs = samples_df[\"sequence\"].tolist()\n",
    "    uniq_overall = 100.0 * (len(set(all_seqs)) / len(all_seqs))\n",
    "\n",
    "    within_stats = {}\n",
    "    print(\"Computing within-condition diversity...\")\n",
    "    for cond_idx, grp in tqdm(samples_df.groupby(\"cond_idx\"), total=samples_df[\"cond_idx\"].nunique()):\n",
    "        seqs = grp[\"sequence\"].tolist()\n",
    "        dists = []\n",
    "        for i in range(len(seqs)):\n",
    "            si = seqs[i]\n",
    "            for j in range(i+1, len(seqs)):\n",
    "                dists.append(normalized_edit_dist(si, seqs[j]))\n",
    "        d = np.array(dists, dtype=float) if dists else np.array([np.nan])\n",
    "        within_stats[cond_idx] = {\n",
    "            \"mean\": float(np.nanmean(d)),\n",
    "            \"std\": float(np.nanstd(d)),\n",
    "            \"median\": float(np.nanmedian(d)),\n",
    "            \"uniq_pct\": 100.0 * (len(set(seqs)) / len(seqs))\n",
    "        }\n",
    "\n",
    "    print(f\"Computing across-condition diversity on {across_pairs_sample} sampled pairs...\")\n",
    "    rng = np.random.default_rng(0)\n",
    "    idxs = samples_df.index.values\n",
    "    d_across = []\n",
    "    tries = 0\n",
    "    while len(d_across) < across_pairs_sample and tries < across_pairs_sample*10:\n",
    "        i, j = rng.integers(0, len(idxs), size=2)\n",
    "        if i == j: \n",
    "            tries += 1; \n",
    "            continue\n",
    "        a = samples_df.iloc[i]\n",
    "        b = samples_df.iloc[j]\n",
    "        if a[\"cond_idx\"] == b[\"cond_idx\"]:\n",
    "            tries += 1\n",
    "            continue\n",
    "        d_across.append(normalized_edit_dist(a[\"sequence\"], b[\"sequence\"]))\n",
    "    d_across = np.array(d_across, dtype=float)\n",
    "    across_stats = {\n",
    "        \"mean\": float(np.mean(d_across)) if len(d_across) else float(\"nan\"),\n",
    "        \"std\": float(np.std(d_across)) if len(d_across) else float(\"nan\"),\n",
    "        \"median\": float(np.median(d_across)) if len(d_across) else float(\"nan\"),\n",
    "        \"n_pairs\": int(len(d_across))\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"uniqueness_overall_pct\": uniq_overall,\n",
    "        \"within_condition\": within_stats,\n",
    "        \"across_condition\": across_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUMBA_AVAILABLE:\n",
    "    @njit(cache=True, fastmath=True)\n",
    "    def _nw_identity_counts_padded(A, LA, i, B, LB, j):\n",
    "        la = int(LA[i]); lb = int(LB[j])\n",
    "        if la == 0 and lb == 0:\n",
    "            return 0, 0\n",
    "        gap = -1\n",
    "        score = np.empty((la+1, lb+1), dtype=np.int16)\n",
    "        match = np.zeros((la+1, lb+1), dtype=np.int16)\n",
    "        alen  = np.zeros((la+1, lb+1), dtype=np.int16)\n",
    "\n",
    "        score[0,0] = 0\n",
    "        for r in range(1, la+1):\n",
    "            score[r,0] = score[r-1,0] + gap\n",
    "            alen[r,0]  = r\n",
    "        for c in range(1, lb+1):\n",
    "            score[0,c] = score[0,c-1] + gap\n",
    "            alen[0,c]  = c\n",
    "\n",
    "        for r in range(1, la+1):\n",
    "            ai = A[i, r-1]\n",
    "            for c in range(1, lb+1):\n",
    "                bj = B[j, c-1]\n",
    "                diag_sc = score[r-1,c-1] + (1 if ai==bj else 0)\n",
    "                diag_mt = match[r-1,c-1] + (1 if ai==bj else 0)\n",
    "                diag_ln = alen[r-1,c-1] + 1\n",
    "\n",
    "                up_sc = score[r-1,c] - 1\n",
    "                up_mt = match[r-1,c]\n",
    "                up_ln = alen[r-1,c] + 1\n",
    "\n",
    "                lf_sc = score[r,c-1] - 1\n",
    "                lf_mt = match[r,c-1]\n",
    "                lf_ln = alen[r,c-1] + 1\n",
    "\n",
    "                sc = diag_sc; mt = diag_mt; ln = diag_ln\n",
    "                if (up_sc > sc) or (up_sc == sc and (up_mt > mt or (up_mt == mt and up_ln < ln))):\n",
    "                    sc = up_sc; mt = up_mt; ln = up_ln\n",
    "                if (lf_sc > sc) or (lf_sc == sc and (lf_mt > mt or (lf_mt == mt and lf_ln < ln))):\n",
    "                    sc = lf_sc; mt = lf_mt; ln = lf_ln\n",
    "\n",
    "                score[r,c] = sc; match[r,c] = mt; alen[r,c] = ln\n",
    "\n",
    "        return int(match[la,lb]), int(alen[la,lb])\n",
    "\n",
    "    @njit(cache=True, fastmath=True)\n",
    "    def _sim_train_block(G, LG, T, LT, i0, i1):\n",
    "        out = np.zeros(i1 - i0, dtype=np.float32)\n",
    "        M = T.shape[0]\n",
    "        for ii in range(i0, i1):\n",
    "            acc = 0.0\n",
    "            for j in range(M):\n",
    "                m, L = _nw_identity_counts_padded(G, LG, ii, T, LT, j)\n",
    "                acc += (1.0 if L == 0 else (m / L))\n",
    "            out[ii - i0] = acc / max(1, M)\n",
    "        return out\n",
    "\n",
    "    @njit(cache=True, fastmath=True)\n",
    "    def _sim_gen_block(G, LG, cond_idx, i0, i1, j0, j1):\n",
    "        \"\"\"\n",
    "        Compute pairwise pID for (i in [i0,i1), j in [j0,j1)), respecting upper-triangle:\n",
    "          - if i0==j0 (diagonal block): only j>i\n",
    "          - otherwise: full cross\n",
    "        Return local sums/counts for i-block and j-block (both all-pairs and within-condition).\n",
    "        \"\"\"\n",
    "        Ni = i1 - i0\n",
    "        Nj = j1 - j0\n",
    "        sums_i_all   = np.zeros(Ni, dtype=np.float64)\n",
    "        counts_i_all = np.zeros(Ni, dtype=np.int32)\n",
    "        sums_j_all   = np.zeros(Nj, dtype=np.float64)\n",
    "        counts_j_all = np.zeros(Nj, dtype=np.int32)\n",
    "\n",
    "        sums_i_in    = np.zeros(Ni, dtype=np.float64)\n",
    "        counts_i_in  = np.zeros(Ni, dtype=np.int32)\n",
    "        sums_j_in    = np.zeros(Nj, dtype=np.float64)\n",
    "        counts_j_in  = np.zeros(Nj, dtype=np.int32)\n",
    "\n",
    "        same_block = (i0 == j0)\n",
    "        for i in range(i0, i1):\n",
    "            ii = i - i0\n",
    "            j_start = i + 1 if same_block else j0\n",
    "            for j in range(j_start, j1):\n",
    "                jj = j - j0\n",
    "                m, L = _nw_identity_counts_padded(G, LG, i, G, LG, j)\n",
    "                pid = 1.0 if L==0 else (m / L)\n",
    "\n",
    "                # all pairs\n",
    "                sums_i_all[ii] += pid; counts_i_all[ii] += 1\n",
    "                sums_j_all[jj] += pid; counts_j_all[jj] += 1\n",
    "\n",
    "                # within-condition\n",
    "                if cond_idx[i] == cond_idx[j]:\n",
    "                    sums_i_in[ii] += pid; counts_i_in[ii] += 1\n",
    "                    sums_j_in[jj] += pid; counts_j_in[jj] += 1\n",
    "\n",
    "        return (sums_i_all, counts_i_all, sums_i_in, counts_i_in,\n",
    "                sums_j_all, counts_j_all, sums_j_in, counts_j_in)\n",
    "\n",
    "else:\n",
    "    pass\n",
    "\n",
    "def annotate_similarity_columns_with_progress(samples_df: pd.DataFrame,\n",
    "                                              train_sequences: list,\n",
    "                                              block_size_sim_train: int = 512,\n",
    "                                              block_size_sim_gen: int = 256,\n",
    "                                              compute_sim_gen_all: bool = True,\n",
    "                                              compute_sim_gen_within: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Exact NW similarity with visible tqdm progress:\n",
    "      - sim_train: averaged vs training (block over i)\n",
    "      - sim_gen_all: averaged vs all other generated (block upper triangle)\n",
    "      - sim_gen_within: averaged vs same-condition only (computed alongside)\n",
    "    \"\"\"\n",
    "    print(\"\\n== Similarity annotation (Njirjak-style) with progress bars ==\")\n",
    "    gen_seqs = samples_df[\"sequence\"].tolist()\n",
    "    cond_idx = samples_df[\"cond_idx\"].to_numpy(dtype=np.int32)\n",
    "\n",
    "    if not NUMBA_AVAILABLE:\n",
    "        print(\"NUMBA not available, skipping similarity computations.\")\n",
    "        return out\n",
    "\n",
    "    print(\"Encoding (padded) sequences...\")\n",
    "    G, LG = encode_pad_sequences(gen_seqs)\n",
    "    T, LT = encode_pad_sequences(train_sequences)\n",
    "    N = G.shape[0]\n",
    "\n",
    "    print(\"Computing sim_train (exact) ...\")\n",
    "    sim_train = np.zeros(N, dtype=np.float32)\n",
    "    n_blocks_i = (N + block_size_sim_train - 1) // block_size_sim_train\n",
    "    for b in tqdm(range(n_blocks_i), desc=\"Sim_train blocks\"):\n",
    "        i0 = b * block_size_sim_train\n",
    "        i1 = min(N, i0 + block_size_sim_train)\n",
    "        sim_train[i0:i1] = _sim_train_block(G, LG, T, LT, i0, i1)\n",
    "\n",
    "    sim_gen_all = np.full(N, np.nan, dtype=np.float32)\n",
    "    sim_gen_within = np.full(N, np.nan, dtype=np.float32)\n",
    "    if compute_sim_gen_all or compute_sim_gen_within:\n",
    "        print(\"Computing sim_gen_all & sim_gen_within (exact) ...\")\n",
    "        sums_all   = np.zeros(N, dtype=np.float64)\n",
    "        counts_all = np.zeros(N, dtype=np.int32)\n",
    "        sums_in    = np.zeros(N, dtype=np.float64)\n",
    "        counts_in  = np.zeros(N, dtype=np.int32)\n",
    "\n",
    "        n_blocks = (N + block_size_sim_gen - 1) // block_size_sim_gen\n",
    "        total_blocks = n_blocks * (n_blocks + 1) // 2\n",
    "\n",
    "        pbar = tqdm(total=total_blocks, desc=\"Sim_gen blocks\")\n",
    "        for bi in range(n_blocks):\n",
    "            i0 = bi * block_size_sim_gen\n",
    "            i1 = min(N, i0 + block_size_sim_gen)\n",
    "            for bj in range(bi, n_blocks):\n",
    "                j0 = bj * block_size_sim_gen\n",
    "                j1 = min(N, j0 + block_size_sim_gen)\n",
    "\n",
    "                (si_all, ci_all, si_in, ci_in,\n",
    "                 sj_all, cj_all, sj_in, cj_in) = _sim_gen_block(G, LG, cond_idx, i0, i1, j0, j1)\n",
    "\n",
    "                sums_all[i0:i1] += si_all\n",
    "                counts_all[i0:i1] += ci_all\n",
    "                sums_in[i0:i1] += si_in\n",
    "                counts_in[i0:i1] += ci_in\n",
    "\n",
    "                sums_all[j0:j1] += sj_all\n",
    "                counts_all[j0:j1] += cj_all\n",
    "                sums_in[j0:j1] += sj_in\n",
    "                counts_in[j0:j1] += cj_in\n",
    "\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "        mask_all = counts_all > 0\n",
    "        sim_gen_all[mask_all] = (sums_all[mask_all] / counts_all[mask_all]).astype(np.float32)\n",
    "        mask_in = counts_in > 0\n",
    "        sim_gen_within[mask_in] = (sums_in[mask_in] / counts_in[mask_in]).astype(np.float32)\n",
    "\n",
    "    out = samples_df.copy()\n",
    "    out[\"sim_train\"] = sim_train\n",
    "    out[\"sim_gen_all\"] = sim_gen_all\n",
    "    out[\"sim_gen_within\"] = sim_gen_within\n",
    "\n",
    "    print(\"sim_train: mean =\", float(np.nanmean(sim_train)), \"median =\", float(np.nanmedian(sim_train)))\n",
    "    if compute_sim_gen_all:\n",
    "        print(\"sim_gen_all: mean =\", float(np.nanmean(sim_gen_all)), \"median =\", float(np.nanmedian(sim_gen_all)))\n",
    "    if compute_sim_gen_within:\n",
    "        print(\"sim_gen_within: mean =\", float(np.nanmean(sim_gen_within)), \"median =\", float(np.nanmedian(sim_gen_within)))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Orchestration -------------------\n",
    "def even_counts_across_lengths(lengths: List[int], total: int) -> Dict[int,int]:\n",
    "    base = total // len(lengths)\n",
    "    rem  = total % len(lengths)\n",
    "    counts = {L: base for L in lengths}\n",
    "    for L in sorted(lengths)[:rem]:\n",
    "        counts[L] += 1\n",
    "    return counts\n",
    "\n",
    "def run_full_evaluation(\n",
    "    cvae_model,\n",
    "    device,\n",
    "    train_df: pd.DataFrame,\n",
    "    train_sequences: List[str],\n",
    "    target_total_in_dist_conditions: int = 100,\n",
    "    per_length_counts: Optional[Dict[int,int]] = None,\n",
    "    total_ood_conditions: int = 10,\n",
    "    n_per_condition: int = 100,\n",
    "    gmm_components: int = 5,\n",
    "    random_state: int = 42,\n",
    "    compute_sim_gen: bool = True,\n",
    "):\n",
    "    lengths = list(range(5, MAX_FASTA_LENGTH+1))\n",
    "\n",
    "    print(\"Fitting per-length GMMs on training conditions...\")\n",
    "    models = fit_gmm_per_length(train_df, lengths, n_components=gmm_components, random_state=random_state)\n",
    "\n",
    "    if per_length_counts is None:\n",
    "        per_length_counts = even_counts_across_lengths(lengths, target_total_in_dist_conditions)\n",
    "    print(\"In-distribution per-length condition counts:\", per_length_counts,\n",
    "          \" Total =\", sum(per_length_counts.values()))\n",
    "\n",
    "    print(\"Sampling in-distribution conditions...\")\n",
    "    in_dist_conds = sample_in_distribution_conditions(models, per_length_counts)\n",
    "\n",
    "    print(f\"Sampling OOD conditions (total={total_ood_conditions})...\")\n",
    "    ood_conds = sample_ood_conditions(train_df, lengths, total_ood=total_ood_conditions)\n",
    "\n",
    "    print(\"Generating in-distribution sequences...\")\n",
    "    gen_in = generate_for_conditions(cvae_model, device, in_dist_conds, n_per_cond=n_per_condition)\n",
    "    print(\"Generating OOD sequences...\")\n",
    "    gen_ood = generate_for_conditions(cvae_model, device, ood_conds, n_per_cond=n_per_condition)\n",
    "\n",
    "    samples_df = pd.DataFrame(gen_in + gen_ood)\n",
    "    print(\"Generation complete. Shape:\", samples_df.shape)\n",
    "    print(samples_df.head())\n",
    "\n",
    "    print(\"\\n== Njirjak Similarity (Exact NW % identity) — per-peptide columns ==\")\n",
    "    samples_df = annotate_similarity_columns_with_progress(\n",
    "        samples_df,\n",
    "        train_sequences=list(train_sequences),\n",
    "        block_size_sim_train=512,\n",
    "        block_size_sim_gen=256,\n",
    "        compute_sim_gen_all=True,\n",
    "        compute_sim_gen_within=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n== Diversity & Uniqueness ==\")\n",
    "    div_stats = compute_diversity_and_uniqueness(samples_df[samples_df[\"ood_type\"]==\"in_dist\"],\n",
    "                                                 across_pairs_sample=100000)\n",
    "    print(\"Overall uniqueness (%):\", div_stats[\"uniqueness_overall_pct\"])\n",
    "    print(\"Across-condition diversity (mean, median, std, n_pairs):\",\n",
    "          div_stats[\"across_condition\"])\n",
    "    \n",
    "    \n",
    "    print(\"\\n== Novelty & Nearest-Train Edit Distance ==\")\n",
    "    novelty_df = compute_exact_novelty_and_nn_edit(samples_df[\"sequence\"].tolist(),\n",
    "                                                   list(train_sequences),\n",
    "                                                   length_window=1)\n",
    "    print(\"Exact novelty (1 - exact_match_rate):\", float(1.0 - novelty_df[\"exact_match\"].mean()))\n",
    "    print(\"Median nearest-train normalized edit distance:\", float(novelty_df[\"nn_dist\"].median()))\n",
    "\n",
    "    samples_df = samples_df.reset_index(drop=True)\n",
    "\n",
    "    novelty_df = compute_exact_novelty_and_nn_edit(\n",
    "        samples_df[\"sequence\"].tolist(),\n",
    "        list(train_sequences),\n",
    "        length_window=1\n",
    "    )\n",
    "\n",
    "    samples_df[\"exact_match\"] = novelty_df[\"exact_match\"].values\n",
    "    samples_df[\"nn_dist\"]     = novelty_df[\"nn_dist\"].values\n",
    "\n",
    "    print(\"\\n== Summary tables ==\")\n",
    "    within_rows = []\n",
    "    for cid, st in div_stats[\"within_condition\"].items():\n",
    "        within_rows.append({\"cond_idx\": cid, **st})\n",
    "    within_df = pd.DataFrame(within_rows).sort_values(\"cond_idx\")\n",
    "\n",
    "    cond_meta = []\n",
    "    for i, rec in enumerate(in_dist_conds):\n",
    "        r = {\"cond_idx\": i, \"ood_type\": \"in_dist\"}\n",
    "        r.update(rec[\"params\"])\n",
    "        cond_meta.append(r)\n",
    "    off = len(in_dist_conds)\n",
    "    for j, rec in enumerate(ood_conds):\n",
    "        r = {\"cond_idx\": off + j, \"ood_type\": \"OOD\"}\n",
    "        r.update(rec[\"params\"])\n",
    "        cond_meta.append(r)\n",
    "    cond_meta_df = pd.DataFrame(cond_meta)\n",
    "\n",
    "    print(\"within_df (first 5):\")\n",
    "    print(within_df.head())\n",
    "    print(\"cond_meta_df (first 5):\")\n",
    "    print(cond_meta_df.head())\n",
    "\n",
    "    results = {\n",
    "        \"samples_df\": samples_df,\n",
    "        \"novelty_df\": novelty_df,\n",
    "        \"diversity_stats\": div_stats,\n",
    "        \"within_df\": within_df,\n",
    "        \"cond_meta_df\": cond_meta_df,\n",
    "    }\n",
    "    print(\"\\nDone.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = set(train_df['peptide'].tolist())\n",
    "train_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting per-length GMMs on training conditions...\n",
      "In-distribution per-length condition counts: {5: 20, 6: 20, 7: 20, 8: 20, 9: 20, 10: 20}  Total = 120\n",
      "Sampling in-distribution conditions...\n",
      "Sampling OOD conditions (total=10)...\n",
      "Length 10: rare beta=1.0, rare hm=0.3156156156156158 (tails 572/569)\n",
      "Length 8: rare beta=0.0, rare hm=0.3547297297297297 (tails 549/536)\n",
      "Length 9: rare beta=1.0, rare hm=0.3342231120008897 (tails 521/521)\n",
      "Length 5: rare beta=0.0, rare hm=0.4284284284284284 (tails 609/588)\n",
      "Length 6: rare beta=0.0, rare hm=0.3928928928928929 (tails 568/551)\n",
      "Length 8: rare beta=0.0, rare hm=0.3547297297297297 (tails 549/536)\n",
      "Length 10: rare beta=1.0, rare hm=0.3156156156156158 (tails 572/569)\n",
      "Length 5: rare beta=0.0, rare hm=0.4284284284284284 (tails 609/588)\n",
      "Length 5: rare beta=0.0, rare hm=0.4284284284284284 (tails 609/588)\n",
      "Length 6: rare beta=0.0, rare hm=0.3928928928928929 (tails 568/551)\n",
      "Generating in-distribution sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sequences per condition: 100%|██████████| 120/120 [00:03<00:00, 34.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating OOD sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sequences per condition: 100%|██████████| 10/10 [00:00<00:00, 42.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation complete. Shape: (13000, 7)\n",
      "   cond_idx    sequence  length  target_length  \\\n",
      "0         0   PCHTNYCGH       9              5   \n",
      "1         0       CSTWA       5              5   \n",
      "2         0    YWISGIYM       8              5   \n",
      "3         0  SYVFQCMFWG      10              5   \n",
      "4         0  FSADFHRWSN      10              5   \n",
      "\n",
      "                                              params  \\\n",
      "0  {'length': 5, 'ap': 0.5175236031701966, 'hydro...   \n",
      "1  {'length': 5, 'ap': 0.5175236031701966, 'hydro...   \n",
      "2  {'length': 5, 'ap': 0.5175236031701966, 'hydro...   \n",
      "3  {'length': 5, 'ap': 0.5175236031701966, 'hydro...   \n",
      "4  {'length': 5, 'ap': 0.5175236031701966, 'hydro...   \n",
      "\n",
      "                                       used_features ood_type  \n",
      "0  [has_beta_sheet_content, is_assembled, net_cha...  in_dist  \n",
      "1  [has_beta_sheet_content, is_assembled, net_cha...  in_dist  \n",
      "2  [has_beta_sheet_content, is_assembled, net_cha...  in_dist  \n",
      "3  [has_beta_sheet_content, is_assembled, net_cha...  in_dist  \n",
      "4  [has_beta_sheet_content, is_assembled, net_cha...  in_dist  \n",
      "\n",
      "== Njirjak Similarity (Exact NW % identity) — per-peptide columns ==\n",
      "\n",
      "== Similarity annotation (Njirjak-style) with progress bars ==\n",
      "Encoding (padded) sequences...\n",
      "Computing sim_train (exact) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sim_train blocks:   0%|          | 0/26 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "run_full_evaluation(\n",
    "    cvae_model=cvae_model,\n",
    "    device=device,\n",
    "    train_df=train_df,\n",
    "    train_sequences=list(train_seqs),\n",
    "    per_length_counts={L:20 for L in range(5, 11)}, \n",
    "    total_ood_conditions=10,\n",
    "    n_per_condition=100,\n",
    "    gmm_components=5,\n",
    "    random_state=42,\n",
    "    compute_sim_gen=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "output_file = \"results/cvae_evaluation_results.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to cvae_evaluation_results_new.pkl\n"
     ]
    }
   ],
   "source": [
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"validate_conditioning_peptides_new.fst\", \"w\") as f:\n",
    "    for i, item in enumerate((results['samples_df'])['sequence'], start=1):\n",
    "        f.write(f\">peptide_{i}\\n{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['samples_df', 'novelty_df', 'diversity_stats', 'within_df', 'cond_meta_df'])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(output_file, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeptidePredictor(\n",
       "  (esm): ESM2(\n",
       "    (embed_tokens): Embedding(33, 480, padding_idx=1)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
       "          (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
       "          (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
       "          (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=480, out_features=1920, bias=True)\n",
       "        (fc2): Linear(in_features=1920, out_features=480, bias=True)\n",
       "        (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (contact_head): ContactPredictionHead(\n",
       "      (regression): Linear(in_features=240, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "    (emb_layer_norm_after): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "    (lm_head): RobertaLMHead(\n",
       "      (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "      (layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (shared): Sequential(\n",
       "    (0): Linear(in_features=480, out_features=128, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ap_head): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (cls_head): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptide_predictor = PeptidePredictor(deepcopy(esm_model_pretrained), alphabet=ALPHABET)\n",
    "\n",
    "peptide_predictor_dict = torch.load(\"../classifier/peptide_predictor.pt\", map_location=device, weights_only=True)\n",
    "peptide_predictor.load_state_dict(peptide_predictor_dict)\n",
    "peptide_predictor.eval()\n",
    "peptide_predictor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_sa_target(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    if isinstance(x, (int, np.integer, float)):\n",
    "        return 1 if int(round(float(x))) != 0 else 0\n",
    "    s = str(x).strip().lower()\n",
    "    if s in {\"sa\",\"yes\",\"true\",\"1\",\"assembled\",\"self-assembly\"}: return 1\n",
    "    if s in {\"no-sa\",\"no\",\"false\",\"0\",\"non-assembly\",\"not\"}:     return 0\n",
    "    return np.nan\n",
    "\n",
    "def _rel_match(pred, targ, tol_abs=0.1):\n",
    "    pred = float(pred); targ = float(targ)\n",
    "    return abs(pred - targ) <= tol_abs\n",
    "\n",
    "@torch.inference_mode()\n",
    "def _predict_ap_sa_aligned(model, sequences, device=\"cuda\", batch_size=2048, sa_threshold=0.5):\n",
    "    \"\"\"Return arrays aligned 1:1 with `sequences` (no merge needed).\"\"\"\n",
    "    model.to(device).eval()\n",
    "    pred_ap, pred_prob, pred_lbl = [], [], []\n",
    "    for start in tqdm(range(0, len(sequences), batch_size), desc=\"Predicting AP/SA\"):\n",
    "        chunk = sequences[start:start+batch_size]\n",
    "        data = [(f\"peptide_{start+i}\", s) for i, s in enumerate(chunk)]\n",
    "        tokens = convert_and_pad(data, seq_length=MAX_SEQ_LENGTH).to(device)\n",
    "        ap_preds, sa_preds = model(tokens)\n",
    "        ap = ap_preds.detach().float().cpu().numpy().ravel()\n",
    "        prob = sa_preds.detach().float().cpu().numpy().ravel()\n",
    "        lbl = (prob >= sa_threshold).astype(np.int32)\n",
    "        pred_ap.extend(ap.tolist()); pred_prob.extend(prob.tolist()); pred_lbl.extend(lbl.tolist())\n",
    "    return np.array(pred_ap), np.array(pred_prob), np.array(pred_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_condition_matches_from_params(\n",
    "    samples_df: pd.DataFrame,\n",
    "    morph: str,\n",
    "    model: torch.nn.Module,\n",
    "    device: str = \"cuda\",\n",
    "    sa_threshold: float = 0.5,\n",
    "    beta_fraction_threshold: float = 0.0,\n",
    "    tol_pct: float = 0.1\n",
    ") -> pd.DataFrame:\n",
    "    df = samples_df.copy()\n",
    "\n",
    "    seqs = df[\"sequence\"].tolist()\n",
    "    pred_ap, pred_sa_prob, pred_is_assembled = _predict_ap_sa_aligned(\n",
    "        model, seqs, device=device, batch_size=2048, sa_threshold=sa_threshold\n",
    "    )\n",
    "    df[\"pred_ap\"] = pred_ap\n",
    "    df[\"pred_sa_prob\"] = pred_sa_prob\n",
    "    df[\"pred_is_assembled\"] = pred_is_assembled\n",
    "\n",
    "    need = [\"sequence\", \"beta_sheet_fraction\", \"hydrophobic_moment\", \"net_charge\"]\n",
    "    missing = [c for c in need if c not in morph.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in metrics CSV: {missing}\")\n",
    "    morph = morph[need].drop_duplicates(subset=[\"sequence\"], keep=\"first\").rename(columns={\n",
    "        \"beta_sheet_fraction\": \"pred_beta_sheet_fraction\",\n",
    "        \"hydrophobic_moment\":  \"pred_hydrophobic_moment\",\n",
    "        \"net_charge\":          \"pred_net_charge\",\n",
    "    })\n",
    "\n",
    "    df = df.merge(morph, on=\"sequence\", how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "    print(df.keys())\n",
    "    df[\"pred_has_beta_sheet_content\"] = df[\"pred_beta_sheet_fraction\"] > float(beta_fraction_threshold)\n",
    "    df[\"seq_length\"] = df[\"sequence\"].str.len().astype(\"Int64\")\n",
    "\n",
    "    def _match_row(row, feat):\n",
    "        used = set(row.get(\"used_features\", []) or [])\n",
    "        if feat not in used:\n",
    "            return np.nan\n",
    "        params = row.get(\"params\", {}) or {}\n",
    "        if feat not in params:\n",
    "            return np.nan\n",
    "\n",
    "        targ = params[feat]\n",
    "\n",
    "        if feat == \"length\":\n",
    "            if pd.isna(row[\"seq_length\"]): return np.nan\n",
    "            try: return int(row[\"seq_length\"]) == int(targ)\n",
    "            except Exception: return np.nan\n",
    "\n",
    "        if feat == \"is_assembled\":\n",
    "            targ_bin = _normalize_sa_target(targ)\n",
    "            if pd.isna(targ_bin) or pd.isna(row[\"pred_is_assembled\"]): return np.nan\n",
    "            return int(row[\"pred_is_assembled\"]) == int(targ_bin)\n",
    "\n",
    "        if feat == \"ap\":\n",
    "            if pd.isna(row[\"pred_ap\"]) or pd.isna(targ): return np.nan\n",
    "            return _rel_match(row[\"pred_ap\"], targ, tol_abs=tol_pct)\n",
    "\n",
    "        if feat == \"has_beta_sheet_content\":\n",
    "            pred = row[\"pred_has_beta_sheet_content\"]\n",
    "            if pd.isna(pred) or pd.isna(targ): return np.nan\n",
    "            try:\n",
    "                targ_bin = bool(int(targ)) if not isinstance(targ, bool) else targ\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "            return bool(pred) == bool(targ_bin)\n",
    "\n",
    "        if feat == \"hydrophobic_moment\":\n",
    "            pred = row[\"pred_hydrophobic_moment\"]\n",
    "            if pd.isna(pred) or pd.isna(targ): return np.nan\n",
    "            return _rel_match(pred, targ, tol_abs=tol_pct)\n",
    "\n",
    "        if feat == \"net_charge\":\n",
    "            pred = row[\"pred_net_charge\"]\n",
    "            if pd.isna(pred) or pd.isna(targ): return np.nan\n",
    "            try: return int(round(float(pred))) == int(round(float(targ)))\n",
    "            except Exception: return np.nan\n",
    "\n",
    "        return np.nan\n",
    "\n",
    "    for feat in FEATURES:\n",
    "        df[f\"match_{feat}\"] = df.apply(lambda r, f=feat: _match_row(r, f), axis=1).astype(\"boolean\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>hydrophobic_moment</th>\n",
       "      <th>net_charge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FSPISEF</td>\n",
       "      <td>0.198770</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DWHCQKDHWY</td>\n",
       "      <td>0.139139</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NWCIYV</td>\n",
       "      <td>0.253587</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HGHMATSS</td>\n",
       "      <td>0.033158</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WSIDY</td>\n",
       "      <td>0.137137</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sequence  hydrophobic_moment  net_charge\n",
       "0     FSPISEF            0.198770    0.416667\n",
       "1  DWHCQKDHWY            0.139139    0.416667\n",
       "2      NWCIYV            0.253587    0.500000\n",
       "3    HGHMATSS            0.033158    0.500000\n",
       "4       WSIDY            0.137137    0.416667"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max = {\n",
    "    #\"has_beta_sheet_content\": (0.0, 1.0),\n",
    "    \"hydrophobic_moment\": (0.000000, 1.998000),\n",
    "    \"net_charge\": (-6.000000, 6.000000),\n",
    "    #\"ap\": (0.959986, 2.897030),\n",
    "}\n",
    "metrics_df = pd.read_csv('gen_peptides/peptide_metrics_validation.csv')\n",
    "\n",
    "if \"sequence\" not in metrics_df.columns:\n",
    "    raise ValueError(\"metrics CSV must contain 'sequence'.\")\n",
    "\n",
    "# normalize metrics_df columns that are in min_max\n",
    "for col, (mn, mx) in min_max.items():\n",
    "    if col not in metrics_df.columns:\n",
    "        raise ValueError(f\"metrics CSV must contain '{col}'.\")\n",
    "    rng = mx - mn\n",
    "    if rng <= 0:\n",
    "        raise ValueError(f\"Invalid min/max for column '{col}': {mn}, {mx}\")\n",
    "    metrics_df[col] = (metrics_df[col] - mn) / rng\n",
    "\n",
    "metrics_df[[\"sequence\"] + list(min_max.keys())].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['peptide_id', 'sequence', 'beta_sheet_fraction', 'extension_ratio',\n",
       "        'hydrophobic_moment', 'planarity_index', 'curvature_score',\n",
       "        'radius_of_gyration', 'alternating_pattern_score', 'net_charge',\n",
       "        'aromatic_interaction_score'],\n",
       "       dtype='object'),\n",
       " Index(['cond_idx', 'sequence', 'length', 'target_length', 'params',\n",
       "        'used_features', 'ood_type', 'sim_train', 'sim_gen_all',\n",
       "        'sim_gen_within', 'exact_match', 'nn_dist', 'pred_ap', 'pred_sa_prob',\n",
       "        'pred_is_assembled', 'metric_beta_sheet_fraction',\n",
       "        'metric_hydrophobic_moment', 'metric_net_charge',\n",
       "        'pred_has_beta_sheet_content', 'seq_length', 'match_length',\n",
       "        'match_is_assembled', 'match_ap', 'match_has_beta_sheet_content',\n",
       "        'match_hydrophobic_moment', 'match_net_charge'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df.keys(), results['samples_df'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting AP/SA:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting AP/SA: 100%|██████████| 7/7 [00:02<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cond_idx', 'sequence', 'length', 'target_length', 'params',\n",
      "       'used_features', 'ood_type', 'sim_train', 'sim_gen_all',\n",
      "       'sim_gen_within', 'exact_match', 'nn_dist', 'pred_ap', 'pred_sa_prob',\n",
      "       'pred_is_assembled', 'metric_beta_sheet_fraction',\n",
      "       'metric_hydrophobic_moment', 'metric_net_charge',\n",
      "       'pred_has_beta_sheet_content', 'seq_length', 'match_length',\n",
      "       'match_is_assembled', 'match_ap', 'match_has_beta_sheet_content',\n",
      "       'match_hydrophobic_moment', 'match_net_charge',\n",
      "       'pred_beta_sheet_fraction', 'pred_hydrophobic_moment',\n",
      "       'pred_net_charge'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "TOL_PCT = 0.10 \n",
    "SA_THRESHOLD = 0.5\n",
    "BETA_FRACTION_THRESHOLD = 0.0\n",
    "\n",
    "results['samples_df'] = add_condition_matches_from_params(\n",
    "    samples_df=results['samples_df'],\n",
    "    morph=metrics_df,\n",
    "    model=peptide_predictor,\n",
    "    device=device,\n",
    "    sa_threshold=SA_THRESHOLD,\n",
    "    beta_fraction_threshold=BETA_FRACTION_THRESHOLD,\n",
    "    tol_pct=TOL_PCT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_list(x):\n",
    "    \"\"\"Robustly turn used_features into a list.\"\"\"\n",
    "    if isinstance(x, list): return x\n",
    "    if pd.isna(x): return []\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            if isinstance(v, (list, tuple)): return list(v)\n",
    "        except Exception:\n",
    "            return [t.strip() for t in x.split(\",\") if t.strip()]\n",
    "    return []\n",
    "\n",
    "def _row_and_across(row, feats):\n",
    "    \"\"\"Return (considered, matched) for a given set of targeted features in this row.\n",
    "       considered=True iff all targeted features have non-NaN match_*.\n",
    "       matched=True iff all those match_* are True.\"\"\"\n",
    "    used = set(_as_list(row.get(\"used_features\", [])))\n",
    "    tgt = [f for f in feats if f in used]\n",
    "    if not tgt:\n",
    "        return False, False\n",
    "    vals = []\n",
    "    for f in tgt:\n",
    "        v = row.get(f\"match_{f}\", pd.NA)\n",
    "        if pd.isna(v):\n",
    "            return False, False\n",
    "        vals.append(bool(v))\n",
    "    return True, all(vals)\n",
    "\n",
    "def compute_condition_matching_effectiveness(samples_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a summary DataFrame with:\n",
    "      - 'metric' : label\n",
    "      - 'k'      : # rows that matched\n",
    "      - 'n'      : # rows considered (had those targets AND non-NaN matches)\n",
    "      - 'effectiveness' : k / n (np.nan if n==0)\n",
    "    \"\"\"\n",
    "    df = samples_df\n",
    "\n",
    "    considered_morph = []\n",
    "    matched_morph = []\n",
    "    for _, r in df.iterrows():\n",
    "        c, m = _row_and_across(r, MORPH_FEATURES)\n",
    "        considered_morph.append(c); matched_morph.append(m)\n",
    "    n_morph = int(np.sum(considered_morph))\n",
    "    k_morph = int(np.sum(np.array(matched_morph)[considered_morph])) if n_morph else 0\n",
    "    eff_morph = (k_morph / n_morph) if n_morph else np.nan\n",
    "\n",
    "    considered_ap = []\n",
    "    matched_ap = []\n",
    "    for _, r in df.iterrows():\n",
    "        used = set(_as_list(r.get(\"used_features\", [])))\n",
    "        if \"ap\" in used:\n",
    "            v = r.get(\"match_ap\", pd.NA)\n",
    "            if not pd.isna(v):\n",
    "                considered_ap.append(True); matched_ap.append(bool(v))\n",
    "            else:\n",
    "                considered_ap.append(False); matched_ap.append(False)\n",
    "        else:\n",
    "            considered_ap.append(False); matched_ap.append(False)\n",
    "    n_ap = int(np.sum(considered_ap))\n",
    "    k_ap = int(np.sum(np.array(matched_ap)[considered_ap])) if n_ap else 0\n",
    "    eff_ap = (k_ap / n_ap) if n_ap else np.nan\n",
    "\n",
    "    considered_sa = []\n",
    "    matched_sa = []\n",
    "    for _, r in df.iterrows():\n",
    "        used = set(_as_list(r.get(\"used_features\", [])))\n",
    "        if \"is_assembled\" in used:\n",
    "            v = r.get(\"match_is_assembled\", pd.NA)\n",
    "            if not pd.isna(v):\n",
    "                considered_sa.append(True); matched_sa.append(bool(v))\n",
    "            else:\n",
    "                considered_sa.append(False); matched_sa.append(False)\n",
    "        else:\n",
    "            considered_sa.append(False); matched_sa.append(False)\n",
    "    n_sa = int(np.sum(considered_sa))\n",
    "    k_sa = int(np.sum(np.array(matched_sa)[considered_sa])) if n_sa else 0\n",
    "    eff_sa = (k_sa / n_sa) if n_sa else np.nan\n",
    "\n",
    "    considered_all = []\n",
    "    matched_all = []\n",
    "    for _, r in df.iterrows():\n",
    "        used = [f for f in FEATURES if f in set(_as_list(r.get(\"used_features\", [])))]\n",
    "        if not used:\n",
    "            considered_all.append(False); matched_all.append(False)\n",
    "            continue\n",
    "        vals = []\n",
    "        ok = True\n",
    "        for f in used:\n",
    "            v = r.get(f\"match_{f}\", pd.NA)\n",
    "            if pd.isna(v):\n",
    "                ok = False; break\n",
    "            vals.append(bool(v))\n",
    "        considered_all.append(ok)\n",
    "        matched_all.append(all(vals) if ok else False)\n",
    "\n",
    "    n_all = int(np.sum(considered_all))\n",
    "    k_all = int(np.sum(np.array(matched_all)[considered_all])) if n_all else 0\n",
    "    eff_all = (k_all / n_all) if n_all else np.nan\n",
    "\n",
    "    summary = pd.DataFrame([\n",
    "        {\"metric\": \"Within 10% of target 3D descriptors\", \"k\": k_morph, \"n\": n_morph, \"effectiveness\": eff_morph},\n",
    "        {\"metric\": \"Within 10% of target AP\",             \"k\": k_ap,    \"n\": n_ap,    \"effectiveness\": eff_ap},\n",
    "        {\"metric\": \"SA/no-SA classification\",             \"k\": k_sa,    \"n\": n_sa,    \"effectiveness\": eff_sa},\n",
    "        {\"metric\": \"Within 10% of all target descriptors\",\"k\": k_all,   \"n\": n_all,   \"effectiveness\": eff_all},\n",
    "    ])\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>used_features</th>\n",
       "      <th>pred_ap</th>\n",
       "      <th>match_ap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>{'length': 5, 'ap': 0.5193244853976117, 'hydro...</td>\n",
       "      <td>[ap, hydrophobic_moment, is_assembled, length,...</td>\n",
       "      <td>0.505081</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>{'length': 5, 'ap': 0.5193244853976117, 'hydro...</td>\n",
       "      <td>[ap, hydrophobic_moment, is_assembled, length,...</td>\n",
       "      <td>0.407708</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>{'length': 5, 'ap': 0.5193244853976117, 'hydro...</td>\n",
       "      <td>[ap, hydrophobic_moment, is_assembled, length,...</td>\n",
       "      <td>0.403653</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>{'length': 5, 'ap': 0.5193244853976117, 'hydro...</td>\n",
       "      <td>[ap, hydrophobic_moment, is_assembled, length,...</td>\n",
       "      <td>0.355600</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>{'length': 5, 'ap': 0.5193244853976117, 'hydro...</td>\n",
       "      <td>[ap, hydrophobic_moment, is_assembled, length,...</td>\n",
       "      <td>0.341355</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12895</th>\n",
       "      <td>{'length': 5, 'hydrophobic_moment': 0.42842842...</td>\n",
       "      <td>[ap, has_beta_sheet_content, hydrophobic_momen...</td>\n",
       "      <td>0.536221</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12896</th>\n",
       "      <td>{'length': 5, 'hydrophobic_moment': 0.42842842...</td>\n",
       "      <td>[ap, has_beta_sheet_content, hydrophobic_momen...</td>\n",
       "      <td>0.589528</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12897</th>\n",
       "      <td>{'length': 5, 'hydrophobic_moment': 0.42842842...</td>\n",
       "      <td>[ap, has_beta_sheet_content, hydrophobic_momen...</td>\n",
       "      <td>0.565194</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12898</th>\n",
       "      <td>{'length': 5, 'hydrophobic_moment': 0.42842842...</td>\n",
       "      <td>[ap, has_beta_sheet_content, hydrophobic_momen...</td>\n",
       "      <td>0.650875</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12899</th>\n",
       "      <td>{'length': 5, 'hydrophobic_moment': 0.42842842...</td>\n",
       "      <td>[ap, has_beta_sheet_content, hydrophobic_momen...</td>\n",
       "      <td>0.558291</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  params  \\\n",
       "100    {'length': 5, 'ap': 0.5193244853976117, 'hydro...   \n",
       "101    {'length': 5, 'ap': 0.5193244853976117, 'hydro...   \n",
       "102    {'length': 5, 'ap': 0.5193244853976117, 'hydro...   \n",
       "103    {'length': 5, 'ap': 0.5193244853976117, 'hydro...   \n",
       "104    {'length': 5, 'ap': 0.5193244853976117, 'hydro...   \n",
       "...                                                  ...   \n",
       "12895  {'length': 5, 'hydrophobic_moment': 0.42842842...   \n",
       "12896  {'length': 5, 'hydrophobic_moment': 0.42842842...   \n",
       "12897  {'length': 5, 'hydrophobic_moment': 0.42842842...   \n",
       "12898  {'length': 5, 'hydrophobic_moment': 0.42842842...   \n",
       "12899  {'length': 5, 'hydrophobic_moment': 0.42842842...   \n",
       "\n",
       "                                           used_features   pred_ap  match_ap  \n",
       "100    [ap, hydrophobic_moment, is_assembled, length,...  0.505081      True  \n",
       "101    [ap, hydrophobic_moment, is_assembled, length,...  0.407708     False  \n",
       "102    [ap, hydrophobic_moment, is_assembled, length,...  0.403653     False  \n",
       "103    [ap, hydrophobic_moment, is_assembled, length,...  0.355600     False  \n",
       "104    [ap, hydrophobic_moment, is_assembled, length,...  0.341355     False  \n",
       "...                                                  ...       ...       ...  \n",
       "12895  [ap, has_beta_sheet_content, hydrophobic_momen...  0.536221      True  \n",
       "12896  [ap, has_beta_sheet_content, hydrophobic_momen...  0.589528      True  \n",
       "12897  [ap, has_beta_sheet_content, hydrophobic_momen...  0.565194      True  \n",
       "12898  [ap, has_beta_sheet_content, hydrophobic_momen...  0.650875     False  \n",
       "12899  [ap, has_beta_sheet_content, hydrophobic_momen...  0.558291      True  \n",
       "\n",
       "[7000 rows x 4 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(results['samples_df'])[['params','used_features', 'pred_ap', 'match_ap']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>k</th>\n",
       "      <th>n</th>\n",
       "      <th>effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Within 10% of target 3D descriptors</td>\n",
       "      <td>6460</td>\n",
       "      <td>11011</td>\n",
       "      <td>0.586686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Within 10% of target AP</td>\n",
       "      <td>5561</td>\n",
       "      <td>7000</td>\n",
       "      <td>0.794429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SA/no-SA classification</td>\n",
       "      <td>6622</td>\n",
       "      <td>8200</td>\n",
       "      <td>0.807561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Within 10% of all target descriptors</td>\n",
       "      <td>6515</td>\n",
       "      <td>12711</td>\n",
       "      <td>0.512548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 metric     k      n  effectiveness\n",
       "0   Within 10% of target 3D descriptors  6460  11011       0.586686\n",
       "1               Within 10% of target AP  5561   7000       0.794429\n",
       "2               SA/no-SA classification  6622   8200       0.807561\n",
       "3  Within 10% of all target descriptors  6515  12711       0.512548"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = compute_condition_matching_effectiveness(results['samples_df'])\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_list(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if pd.isna(x): return []\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            if isinstance(v, (list, tuple)): return list(v)\n",
    "        except Exception:\n",
    "            return [t.strip() for t in x.split(\",\") if t.strip()]\n",
    "    return []\n",
    "\n",
    "def _row_and_across(row, feats):\n",
    "    \"\"\"(considered, matched) for AND across the targeted subset of feats in this row.\"\"\"\n",
    "    used = set(_as_list(row.get(\"used_features\", [])))\n",
    "    tgt = [f for f in feats if f in used]\n",
    "    if not tgt:\n",
    "        return False, False\n",
    "    vals = []\n",
    "    for f in tgt:\n",
    "        v = row.get(f\"match_{f}\", pd.NA)\n",
    "        if pd.isna(v):\n",
    "            return False, False\n",
    "        vals.append(bool(v))\n",
    "    return True, all(vals)\n",
    "\n",
    "def _row_single(row, feat):\n",
    "    \"\"\"(considered, matched) for a single feature.\"\"\"\n",
    "    used = set(_as_list(row.get(\"used_features\", [])))\n",
    "    if feat not in used:\n",
    "        return False, False\n",
    "    v = row.get(f\"match_{feat}\", pd.NA)\n",
    "    if pd.isna(v):\n",
    "        return False, False\n",
    "    return True, bool(v)\n",
    "\n",
    "def compute_condition_matching_effectiveness_by_k(samples_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a summary with columns:\n",
    "      - num_conditioned : k = number of used (targeted) descriptors among FEATURES\n",
    "      - metric          : label\n",
    "      - k_match         : # rows matched (successes)\n",
    "      - n_considered    : # rows considered (had those targets AND non-NaN matches)\n",
    "      - effectiveness   : k_match / n_considered (NaN if n_considered==0)\n",
    "    \"\"\"\n",
    "    df = samples_df.copy()\n",
    "\n",
    "    used_counts = []\n",
    "    cons_morph, match_morph = [], []\n",
    "    cons_ap, match_ap = [], []\n",
    "    cons_sa, match_sa = [], []\n",
    "    cons_all, match_all = [], []\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        used = set(_as_list(r.get(\"used_features\", [])))\n",
    "        used_in_scope = [f for f in FEATURES if f in used]\n",
    "        used_counts.append(len(used_in_scope))\n",
    "\n",
    "        c_m, m_m = _row_and_across(r, MORPH_FEATURES)\n",
    "        cons_morph.append(c_m); match_morph.append(m_m)\n",
    "\n",
    "        c_a, m_a = _row_single(r, \"ap\")\n",
    "        cons_ap.append(c_a); match_ap.append(m_a)\n",
    "\n",
    "        c_s, m_s = _row_single(r, \"is_assembled\")\n",
    "        cons_sa.append(c_s); match_sa.append(m_s)\n",
    "\n",
    "        if not used_in_scope:\n",
    "            cons_all.append(False); match_all.append(False)\n",
    "        else:\n",
    "            ok = True; vals = []\n",
    "            for f in used_in_scope:\n",
    "                v = r.get(f\"match_{f}\", pd.NA)\n",
    "                if pd.isna(v):\n",
    "                    ok = False; break\n",
    "                vals.append(bool(v))\n",
    "            cons_all.append(ok)\n",
    "            match_all.append(all(vals) if ok else False)\n",
    "\n",
    "    df[\"_k_used\"] = used_counts\n",
    "    df[\"_c_morph\"] = cons_morph; df[\"_m_morph\"] = match_morph\n",
    "    df[\"_c_ap\"]    = cons_ap;    df[\"_m_ap\"]    = match_ap\n",
    "    df[\"_c_sa\"]    = cons_sa;    df[\"_m_sa\"]    = match_sa\n",
    "    df[\"_c_all\"]   = cons_all;   df[\"_m_all\"]   = match_all\n",
    "\n",
    "    rows = []\n",
    "    for k_val, g in df.groupby(\"_k_used\", dropna=False):\n",
    "        if k_val == 0:\n",
    "            continue\n",
    "\n",
    "        def cm(c_col, m_col):\n",
    "            c = g[c_col].to_numpy(dtype=bool)\n",
    "            m = g[m_col].to_numpy(dtype=bool)\n",
    "            n = int(c.sum())\n",
    "            k = int(m[c].sum()) if n else 0\n",
    "            eff = (k / n) if n else np.nan\n",
    "            return k, n, eff\n",
    "\n",
    "        k_morph, n_morph, eff_morph = cm(\"_c_morph\", \"_m_morph\")\n",
    "        k_ap,    n_ap,    eff_ap    = cm(\"_c_ap\",    \"_m_ap\")\n",
    "        k_sa,    n_sa,    eff_sa    = cm(\"_c_sa\",    \"_m_sa\")\n",
    "        k_all,   n_all,   eff_all   = cm(\"_c_all\",   \"_m_all\")\n",
    "\n",
    "        rows.extend([\n",
    "            {\"num_conditioned\": k_val, \"metric\": \"Within 10% of target 3D descriptors\",\n",
    "             \"k_match\": k_morph, \"n_considered\": n_morph, \"effectiveness\": eff_morph},\n",
    "            {\"num_conditioned\": k_val, \"metric\": \"Within 10% of target AP\",\n",
    "             \"k_match\": k_ap,    \"n_considered\": n_ap,    \"effectiveness\": eff_ap},\n",
    "            {\"num_conditioned\": k_val, \"metric\": \"SA/no-SA classification\",\n",
    "             \"k_match\": k_sa,    \"n_considered\": n_sa,    \"effectiveness\": eff_sa},\n",
    "            {\"num_conditioned\": k_val, \"metric\": \"Within 10% of all target descriptors\",\n",
    "             \"k_match\": k_all,   \"n_considered\": n_all,   \"effectiveness\": eff_all},\n",
    "        ])\n",
    "\n",
    "    out = pd.DataFrame(rows).sort_values([\"num_conditioned\", \"metric\"]).reset_index(drop=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_conditioned</th>\n",
       "      <th>metric</th>\n",
       "      <th>k_match</th>\n",
       "      <th>n_considered</th>\n",
       "      <th>effectiveness</th>\n",
       "      <th>effectiveness_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>SA/no-SA classification</td>\n",
       "      <td>550</td>\n",
       "      <td>600</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>91.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Within 10% of all target descriptors</td>\n",
       "      <td>1980</td>\n",
       "      <td>2341</td>\n",
       "      <td>0.845792</td>\n",
       "      <td>84.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Within 10% of target 3D descriptors</td>\n",
       "      <td>647</td>\n",
       "      <td>941</td>\n",
       "      <td>0.687566</td>\n",
       "      <td>68.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Within 10% of target AP</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>83.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>SA/no-SA classification</td>\n",
       "      <td>776</td>\n",
       "      <td>900</td>\n",
       "      <td>0.862222</td>\n",
       "      <td>86.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Within 10% of all target descriptors</td>\n",
       "      <td>1280</td>\n",
       "      <td>1995</td>\n",
       "      <td>0.641604</td>\n",
       "      <td>64.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Within 10% of target 3D descriptors</td>\n",
       "      <td>1213</td>\n",
       "      <td>1695</td>\n",
       "      <td>0.715634</td>\n",
       "      <td>71.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Within 10% of target AP</td>\n",
       "      <td>431</td>\n",
       "      <td>600</td>\n",
       "      <td>0.718333</td>\n",
       "      <td>71.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>SA/no-SA classification</td>\n",
       "      <td>865</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>86.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>Within 10% of all target descriptors</td>\n",
       "      <td>1018</td>\n",
       "      <td>1908</td>\n",
       "      <td>0.533543</td>\n",
       "      <td>53.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>Within 10% of target 3D descriptors</td>\n",
       "      <td>1199</td>\n",
       "      <td>1908</td>\n",
       "      <td>0.628407</td>\n",
       "      <td>62.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>Within 10% of target AP</td>\n",
       "      <td>764</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>76.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>SA/no-SA classification</td>\n",
       "      <td>1237</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.824667</td>\n",
       "      <td>82.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>Within 10% of all target descriptors</td>\n",
       "      <td>863</td>\n",
       "      <td>1974</td>\n",
       "      <td>0.437183</td>\n",
       "      <td>43.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>Within 10% of target 3D descriptors</td>\n",
       "      <td>1231</td>\n",
       "      <td>1974</td>\n",
       "      <td>0.623607</td>\n",
       "      <td>62.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>Within 10% of target AP</td>\n",
       "      <td>977</td>\n",
       "      <td>1300</td>\n",
       "      <td>0.751538</td>\n",
       "      <td>75.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>SA/no-SA classification</td>\n",
       "      <td>1548</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.774000</td>\n",
       "      <td>77.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>Within 10% of all target descriptors</td>\n",
       "      <td>834</td>\n",
       "      <td>2293</td>\n",
       "      <td>0.363716</td>\n",
       "      <td>36.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>Within 10% of target 3D descriptors</td>\n",
       "      <td>1259</td>\n",
       "      <td>2293</td>\n",
       "      <td>0.549062</td>\n",
       "      <td>54.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>Within 10% of target AP</td>\n",
       "      <td>1433</td>\n",
       "      <td>1800</td>\n",
       "      <td>0.796111</td>\n",
       "      <td>79.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>SA/no-SA classification</td>\n",
       "      <td>1646</td>\n",
       "      <td>2200</td>\n",
       "      <td>0.748182</td>\n",
       "      <td>74.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6</td>\n",
       "      <td>Within 10% of all target descriptors</td>\n",
       "      <td>540</td>\n",
       "      <td>2200</td>\n",
       "      <td>0.245455</td>\n",
       "      <td>24.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>Within 10% of target 3D descriptors</td>\n",
       "      <td>911</td>\n",
       "      <td>2200</td>\n",
       "      <td>0.414091</td>\n",
       "      <td>41.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>Within 10% of target AP</td>\n",
       "      <td>1873</td>\n",
       "      <td>2200</td>\n",
       "      <td>0.851364</td>\n",
       "      <td>85.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_conditioned                                metric  k_match  \\\n",
       "0                 1               SA/no-SA classification      550   \n",
       "1                 1  Within 10% of all target descriptors     1980   \n",
       "2                 1   Within 10% of target 3D descriptors      647   \n",
       "3                 1               Within 10% of target AP       83   \n",
       "4                 2               SA/no-SA classification      776   \n",
       "5                 2  Within 10% of all target descriptors     1280   \n",
       "6                 2   Within 10% of target 3D descriptors     1213   \n",
       "7                 2               Within 10% of target AP      431   \n",
       "8                 3               SA/no-SA classification      865   \n",
       "9                 3  Within 10% of all target descriptors     1018   \n",
       "10                3   Within 10% of target 3D descriptors     1199   \n",
       "11                3               Within 10% of target AP      764   \n",
       "12                4               SA/no-SA classification     1237   \n",
       "13                4  Within 10% of all target descriptors      863   \n",
       "14                4   Within 10% of target 3D descriptors     1231   \n",
       "15                4               Within 10% of target AP      977   \n",
       "16                5               SA/no-SA classification     1548   \n",
       "17                5  Within 10% of all target descriptors      834   \n",
       "18                5   Within 10% of target 3D descriptors     1259   \n",
       "19                5               Within 10% of target AP     1433   \n",
       "20                6               SA/no-SA classification     1646   \n",
       "21                6  Within 10% of all target descriptors      540   \n",
       "22                6   Within 10% of target 3D descriptors      911   \n",
       "23                6               Within 10% of target AP     1873   \n",
       "\n",
       "    n_considered  effectiveness  effectiveness_pct  \n",
       "0            600       0.916667              91.67  \n",
       "1           2341       0.845792              84.58  \n",
       "2            941       0.687566              68.76  \n",
       "3            100       0.830000              83.00  \n",
       "4            900       0.862222              86.22  \n",
       "5           1995       0.641604              64.16  \n",
       "6           1695       0.715634              71.56  \n",
       "7            600       0.718333              71.83  \n",
       "8           1000       0.865000              86.50  \n",
       "9           1908       0.533543              53.35  \n",
       "10          1908       0.628407              62.84  \n",
       "11          1000       0.764000              76.40  \n",
       "12          1500       0.824667              82.47  \n",
       "13          1974       0.437183              43.72  \n",
       "14          1974       0.623607              62.36  \n",
       "15          1300       0.751538              75.15  \n",
       "16          2000       0.774000              77.40  \n",
       "17          2293       0.363716              36.37  \n",
       "18          2293       0.549062              54.91  \n",
       "19          1800       0.796111              79.61  \n",
       "20          2200       0.748182              74.82  \n",
       "21          2200       0.245455              24.55  \n",
       "22          2200       0.414091              41.41  \n",
       "23          2200       0.851364              85.14  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_k = compute_condition_matching_effectiveness_by_k(results['samples_df'])\n",
    "by_k.assign(effectiveness_pct=(by_k[\"effectiveness\"]*100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _as_list(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            if isinstance(v, (list, tuple)): return list(v)\n",
    "        except Exception:\n",
    "            return [t.strip() for t in x.split(\",\") if t.strip()]\n",
    "    return [] if pd.isna(x) else []\n",
    "\n",
    "def _row_success(row):\n",
    "    used = [f for f in FEATURES if f in set(_as_list(row.get(\"used_features\", [])))]\n",
    "    if not used:\n",
    "        return False\n",
    "    if \"match_all_targeted\" in row and not pd.isna(row[\"match_all_targeted\"]):\n",
    "        return bool(row[\"match_all_targeted\"])\n",
    "    for f in used:\n",
    "        v = row.get(f\"match_\"+f, pd.NA)\n",
    "        if pd.isna(v) or not bool(v):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def _count_k(u):\n",
    "    used = set(_as_list(u))\n",
    "    return sum(f in used for f in COUNT_FEATURES)\n",
    "\n",
    "tmp = results['samples_df'].copy()\n",
    "tmp = tmp[tmp[\"ood_type\"] == \"in_dist\"].copy()\n",
    "tmp[\"k\"] = tmp[\"used_features\"].apply(_count_k)\n",
    "tmp[\"success\"] = tmp.apply(_row_success, axis=1)\n",
    "\n",
    "plot_df = (tmp[tmp[\"k\"] > 0]\n",
    "           .groupby(\"k\", as_index=False)\n",
    "           .agg(total=(\"success\",\"size\"), successes=(\"success\",\"sum\")))\n",
    "plot_df[\"remainder\"] = plot_df[\"total\"] - plot_df[\"successes\"]\n",
    "\n",
    "max_k = len(COUNT_FEATURES) if len(COUNT_FEATURES) > 0 else plot_df[\"k\"].max()\n",
    "full_k = np.arange(1, max_k+1, dtype=int)\n",
    "plot_df = (plot_df.set_index(\"k\")\n",
    "                  .reindex(full_k, fill_value=0)\n",
    "                  .rename_axis(\"k\").reset_index())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4.8, 3.5))\n",
    "\n",
    "x = plot_df[\"k\"].to_numpy()\n",
    "succ = plot_df[\"successes\"].to_numpy()\n",
    "rem  = plot_df[\"remainder\"].to_numpy()\n",
    "\n",
    "SUCCESS_COLOR = TEAL[4]\n",
    "REMAINDER_COLOR = TEAL[0]\n",
    "BAR_WIDTH = 0.7\n",
    "\n",
    "ax.bar(x, succ, width=BAR_WIDTH, label=\"Matching peptides\",\n",
    "       color=SUCCESS_COLOR, alpha=0.9, linewidth=0)\n",
    "\n",
    "ax.bar(x, rem, width=BAR_WIDTH, bottom=succ, label=\"All peptides\",\n",
    "       color=REMAINDER_COLOR, alpha=0.9, linewidth=0)\n",
    "\n",
    "for spine in (\"top\", \"right\"):\n",
    "    ax.spines[spine].set_visible(False)\n",
    "\n",
    "ax.set_xticks(full_k)\n",
    "ax.set_xlim(full_k.min()-0.6, full_k.max()+0.6)\n",
    "\n",
    "leg = ax.legend(\n",
    "    loc=\"upper right\",\n",
    "    bbox_to_anchor=(0.965, 0.9),\n",
    "    ncol=1,\n",
    "    frameon=True, fancybox=True, framealpha=0.9, fontsize=10\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"conditions_success_stacked.svg\", format=\"svg\", bbox_inches=\"tight\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to cvae_evaluation_results_new.pkl\n"
     ]
    }
   ],
   "source": [
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
